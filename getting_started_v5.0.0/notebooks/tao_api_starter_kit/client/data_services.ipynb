{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAO remote client - Data-Services\n",
    "### The workflow in a nutshell\n",
    "TAO Data Services include 4 key pipelines:\n",
    "1. Offline data augmentation using DALI\n",
    "2. Auto labeling using TAO Mask Auto-labeler (MAL)\n",
    "3. Annotation conversion\n",
    "4. Groundtruth analytics\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn how to leverage the simplicity and convenience of TAO to:\n",
    "\n",
    "* Convert KITTI dataset to COCO format\n",
    "* Run auto-labeling to generate pseudo masks for KITTI bounding boxes\n",
    "* Apply data augmentation to the KITTI dataset with bounding boxe refinement\n",
    "* Run data analytics to collect useful statistics on the original and augmented KITTI dataset\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Convert KITTI data to COCO format](#head-1)\n",
    "2. [Generate pseudo-masks with the auto-labeler](#head-2)\n",
    "3. [Apply data augmentation](#head-3)\n",
    "4. [Perform data analytics](#head-4)\n",
    "\n",
    "\n",
    "### Requirements\n",
    "Please find the server requirements [here](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import getpass\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "namespace = 'default'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install TAO remote client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # SKIP this step IF you have already installed the TAO-Client wheel.\n",
    "! pip3 install nvidia-tao-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # View the version of the TAO-Client\n",
    "! tao-client --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the remote service base URL and Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXME\n",
    "\n",
    "1. Assign the ip_address and port_number in FIXME 1 and FIXME 2 ([info](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_rest_api.html))\n",
    "2. Assign the ngc_api_key variable in FIXME 3\n",
    "3. Assign path of DATA_DIR in FIXME 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the node_addr and port number\n",
    "node_addr = \"<ip_address>\"  # FIXME 1 example: 10.137.149.22\n",
    "node_port = \"<port_number>\"  # FIXME 2 example: 32334\n",
    "# In host machine, node IP address and port number can be obtained as follows,\n",
    "# node_addr: hostname -I\n",
    "# node_port: kubectl get service ingress-nginx-controller -o jsonpath='{.spec.ports[0].nodePort}'\n",
    "ngc_api_key = \"<ngc_api_key>\"  # FIXME 3 example: (Add NGC API key)\n",
    "data_dir = \"<DATA_DIR>\" # FIXME4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env BASE_URL=http://{node_addr}:{node_port}/api/v1\n",
    "\n",
    "# Exchange NGC_API_KEY for JWT\n",
    "identity = json.loads(subprocess.getoutput(f'tao-client login --ngc-api-key {ngc_api_key}'))\n",
    "\n",
    "%env USER={identity['user_id']}\n",
    "%env TOKEN={identity['token']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the shared volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get PVC ID\n",
    "pvc_id = subprocess.getoutput(f'kubectl get pvc tao-toolkit-api-pvc -n {namespace} -o jsonpath=\"{{.spec.volumeName}}\"')\n",
    "print(pvc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get NFS server info\n",
    "provisioner = json.loads(subprocess.getoutput(f'helm get values nfs-subdir-external-provisioner -o json'))\n",
    "nfs_server = provisioner['nfs']['server']\n",
    "nfs_path = provisioner['nfs']['path']\n",
    "print(nfs_server, nfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user = getpass.getuser()\n",
    "home = os.path.expanduser('~')\n",
    "\n",
    "! echo \"Password for {user}\"\n",
    "password = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mount shared volume \n",
    "! mkdir -p ~/shared\n",
    "\n",
    "command = \"apt-get -y install nfs-common >> /dev/null\"\n",
    "! echo {password} | sudo -S -k {command}\n",
    "\n",
    "command = f\"mount -t nfs {nfs_server}:{nfs_path}/{namespace}-tao-toolkit-api-pvc-{pvc_id} ~/shared\"\n",
    "! echo {password} | sudo -S -k {command} && echo DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convert KITTI data to COCO format <a class=\"anchor\" id=\"head-1\"></a>\n",
    "We would first convert the dataset from KITTI to COCO formats.\n",
    "\n",
    "### Define the task and action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"annotations\"\n",
    "action = \"convert\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset\n",
    "We support both KITTI and COCO data formats\n",
    "\n",
    "KITTI dataset follow the directory structure displayed below:\n",
    "```\n",
    "$DATA_DIR/dataset\n",
    "├── images\n",
    "│   ├── image_name_1.jpg\n",
    "│   ├── image_name_2.jpg\n",
    "|   ├── ...\n",
    "└── labels\n",
    "    ├── image_name_1.txt\n",
    "    ├── image_name_2.txt\n",
    "    ├── ...\n",
    "```\n",
    "\n",
    "And COCO dataset follow the directory structure displayed below:\n",
    "```\n",
    "$DATA_DIR/dataset\n",
    "├── images\n",
    "│   ├── image_name_1.jpg\n",
    "│   ├── image_name_2.jpg\n",
    "|   ├── ...\n",
    "└── annotations.json\n",
    "```\n",
    "For this notebook, we will be using the KITTI object detection dataset for this example. To find more details, please visit [here](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=2d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset Links\n",
    "images_url = \"https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_image_2.zip\"\n",
    "labels_url = \"https://s3.eu-central-1.amazonaws.com/avg-kitti/data_object_label_2.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "!wget -O images.zip {images_url}\n",
    "!wget -O labels.zip {labels_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip -q images.zip -d {data_dir}/\n",
    "!unzip -q labels.zip -d {data_dir}/\n",
    "!mkdir -p {data_dir}/images {data_dir}/labels\n",
    "!mv {data_dir}/training/image_2/000* {data_dir}/images/\n",
    "!mv {data_dir}/training/label_2/000* {data_dir}/labels/\n",
    "!cd {data_dir} && tar -cf kitti_dataset.tar images labels\n",
    "!rm -rf images.zip labels.zip {data_dir}/training/ {data_dir}/training/ {data_dir}/testing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset_id = subprocess.getoutput(f\"tao-client {model_name} dataset-create --dataset_type object_detection --dataset_format raw\")\n",
    "print(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rsync -ah --info=progress2 {data_dir}/images ~/shared/users/{identity['user_id']}/datasets/{dataset_id}/\n",
    "!rsync -ah --info=progress2 {data_dir}/labels ~/shared/users/{identity['user_id']}/datasets/{dataset_id}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the created datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = os.path.join(home, 'shared', 'users', os.environ['USER'], 'datasets', '*', 'metadata.json')\n",
    "\n",
    "datasets = []\n",
    "for metadata_path in glob.glob(pattern):\n",
    "    with open(metadata_path, 'r') as metadata_file:\n",
    "        datasets.append(json.load(metadata_file))\n",
    "\n",
    "print(json.dumps(datasets, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model_id = subprocess.getoutput(f\"tao-client {model_name} model-create --network_arch {model_name}\")\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign datast\n",
    "metadata_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'metadata.json')\n",
    "\n",
    "with open(metadata_path , \"r\") as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "metadata[\"inference_dataset\"] = dataset_id\n",
    "\n",
    "with open(metadata_path, \"w\") as metadata_file:\n",
    "    json.dump(metadata, metadata_file, indent=2)\n",
    "\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set action specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model specs\n",
    "! tao-client {model_name} model-action-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/{action}.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', f'{action}.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "# Updating specs\n",
    "specs[\"data\"][\"input_format\"] = \"KITTI\"\n",
    "specs[\"data\"][\"output_format\"] = \"COCO\"\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the data format conversion action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "convert_job_id = subprocess.getoutput(f\"tao-client {model_name} execute-action --id {model_id}\")\n",
    "print(convert_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "logs_dir = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'logs')\n",
    "log_file = f\"{convert_job_id}.txt\"\n",
    "\n",
    "def my_tail(logs_dir, log_file):\n",
    "    %env LOG_FILE={logs_dir}/{log_file}\n",
    "    ! mkdir -p {logs_dir}\n",
    "    ! [ ! -f \"$LOG_FILE\" ] && touch $LOG_FILE && chmod 666 $LOG_FILE\n",
    "    ! tail -f -n +1 $LOG_FILE | while read LINE; do echo \"$LINE\"; [[ \"$LINE\" == \"EOF\" ]] && pkill -P $$ tail; done\n",
    "    \n",
    "\n",
    "my_tail(logs_dir, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the COCO annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy annotations to the dataset\n",
    "!rsync -ah --info=progress2 ~/shared/users/{identity['user_id']}/models/{model_id}/{convert_job_id}/{dataset_id}.json {data_dir}/annotations.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf ~/shared/users/{os.environ['USER']}/models/{model_id}\n",
    "! echo DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf ~/shared/users/{os.environ['USER']}/datasets/{dataset_id}\n",
    "! echo DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate pseudo-masks with the auto-labeler <a class=\"anchor\" id=\"head-2\"></a>\n",
    "Here we will use a pretrained MAL model to generate pseudo-masks for the converted KITTI data. \n",
    "\n",
    "### Define the task and action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"auto-label\"\n",
    "action = \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset\n",
    "We would be formatting the original dataset to include the COCO annotations generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "dataset_id = subprocess.getoutput(f\"tao-client {model_name} dataset-create --dataset_type object_detection --dataset_format raw\")\n",
    "print(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rsync -ah --info=progress2 {data_dir}/images ~/shared/users/{identity['user_id']}/datasets/{dataset_id}/\n",
    "!rsync -ah --info=progress2 {data_dir}/annotations.json ~/shared/users/{identity['user_id']}/datasets/{dataset_id}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = os.path.join(home, 'shared', 'users', os.environ['USER'], 'datasets', '*', 'metadata.json')\n",
    "\n",
    "datasets = []\n",
    "for metadata_path in glob.glob(pattern):\n",
    "    with open(metadata_path, 'r') as metadata_file:\n",
    "        datasets.append(json.load(metadata_file))\n",
    "\n",
    "print(json.dumps(datasets, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "network_arch = model_name.replace(\"-\",\"_\")\n",
    "model_id = subprocess.getoutput(f\"tao-client {model_name} model-create --network_arch {network_arch}\")\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the PTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all pretrained models for the chosen network architecture\n",
    "pattern = os.path.join(home, 'shared', 'users', '*', 'models', '*', 'metadata.json')\n",
    "\n",
    "for ptm_metadata_path in glob.glob(pattern):\n",
    "  with open(ptm_metadata_path, 'r') as metadata_file:\n",
    "    ptm_metadata = json.load(metadata_file)\n",
    "    metadata_network_arch = ptm_metadata.get(\"network_arch\")\n",
    "    if metadata_network_arch == network_arch:\n",
    "        print(f'PTM Name: {ptm_metadata[\"name\"]}; PTM version: {ptm_metadata[\"version\"]}; NGC PATH: {ptm_metadata[\"ngc_path\"]}; Additional info: {ptm_metadata[\"additional_id_info\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_map = {\"auto_label\" : \"mask_auto_label:trainable_v1.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained model\n",
    "ptm = []\n",
    "for ptm_metadata_path in glob.glob(pattern):\n",
    "    with open(ptm_metadata_path, 'r') as metadata_file:\n",
    "        ptm_metadata = json.load(metadata_file)\n",
    "        ngc_path = ptm_metadata.get(\"ngc_path\")\n",
    "        metadata_network_arch = ptm_metadata.get(\"network_arch\")\n",
    "        if metadata_network_arch == network_arch and ngc_path.endswith(pretrained_map[network_arch]):\n",
    "            ptm = [ptm_metadata[\"id\"]]\n",
    "            break\n",
    "\n",
    "print(ptm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the PTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign PTM\n",
    "metadata_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'metadata.json')\n",
    "\n",
    "with open(metadata_path , \"r\") as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "metadata[\"ptm\"] = ptm\n",
    "\n",
    "with open(metadata_path, \"w\") as metadata_file:\n",
    "    json.dump(metadata, metadata_file, indent=2)\n",
    "\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign datast\n",
    "metadata_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'metadata.json')\n",
    "\n",
    "with open(metadata_path , \"r\") as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "metadata[\"inference_dataset\"] = dataset_id\n",
    "\n",
    "with open(metadata_path, \"w\") as metadata_file:\n",
    "    json.dump(metadata, metadata_file, indent=2)\n",
    "\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set action specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default train model specs\n",
    "! tao-client {model_name} model-action-defaults --id {model_id} | tee ~/shared/users/{identity['user_id']}/models/{model_id}/specs/{action}.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', f'{action}.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "# Override any of the parameters listed in the previous cell as required\n",
    "specs[\"gpu_ids\"] = [0]\n",
    "    \n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "label_job_id = subprocess.getoutput(f\"tao-client {model_name} execute-action --id {model_id}\")\n",
    "print(label_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "logs_dir = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'logs')\n",
    "log_file = f\"{label_job_id}.txt\"\n",
    "\n",
    "my_tail(logs_dir, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the label masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy annotations to the dataset\n",
    "!rsync -ah --info=progress2 ~/shared/users/{identity['user_id']}/models/{model_id}/{label_job_id}/label.json {data_dir}/label.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf ~/shared/users/{os.environ['USER']}/models/{model_id}\n",
    "! echo DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf ~/shared/users/{os.environ['USER']}/datasets/{dataset_id}\n",
    "! echo DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply data augmentation <a class=\"anchor\" id=\"head-3\"></a>\n",
    "In this section, we run offline augmentation with the original dataset. During the augmentation process, we can use the pseudo-masks generated from the last step to refine the distorted or rotated bounding boxes.\n",
    "\n",
    "### Define the task and action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"augmentation\"\n",
    "action = \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset\n",
    "We would be formatting the dataset to include the generated mask information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset_id = subprocess.getoutput(f\"tao-client {model_name} dataset-create --dataset_type object_detection --dataset_format raw\")\n",
    "print(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rsync -ah --info=progress2 {data_dir}/images ~/shared/users/{identity['user_id']}/datasets/{dataset_id}/\n",
    "!rsync -ah --info=progress2 {data_dir}/labels ~/shared/users/{identity['user_id']}/datasets/{dataset_id}/\n",
    "!rsync -ah --info=progress2 {data_dir}/label.json ~/shared/users/{identity['user_id']}/datasets/{dataset_id}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = os.path.join(home, 'shared', 'users', os.environ['USER'], 'datasets', '*', 'metadata.json')\n",
    "\n",
    "datasets = []\n",
    "for metadata_path in glob.glob(pattern):\n",
    "    with open(metadata_path, 'r') as metadata_file:\n",
    "        datasets.append(json.load(metadata_file))\n",
    "\n",
    "print(json.dumps(datasets, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model_id = subprocess.getoutput(f\"tao-client {model_name} model-create --network_arch {model_name}\")\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign datast\n",
    "metadata_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'metadata.json')\n",
    "\n",
    "with open(metadata_path , \"r\") as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "metadata[\"inference_dataset\"] = dataset_id\n",
    "\n",
    "with open(metadata_path, \"w\") as metadata_file:\n",
    "    json.dump(metadata, metadata_file, indent=2)\n",
    "\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set action specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model specs\n",
    "! tao-client {model_name} model-action-defaults --id {model_id} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/{action}.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', f'{action}.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the data augmentation action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "augment_job_id = subprocess.getoutput(f\"tao-client {model_name} execute-action --id {model_id}\")\n",
    "print(augment_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "logs_dir = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'logs')\n",
    "log_file = f\"{augment_job_id}.txt\"\n",
    "\n",
    "my_tail(logs_dir, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf ~/shared/users/{os.environ['USER']}/models/{model_id}\n",
    "! echo DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf ~/shared/users/{os.environ['USER']}/datasets/{dataset_id}\n",
    "! echo DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform data analytics  <a class=\"anchor\" id=\"head-4\"></a>\n",
    "Next, we perform analytics with the KITTI dataset.\n",
    "\n",
    "### Assigning the task and action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"analytics\"\n",
    "action = \"analyze\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset_id = subprocess.getoutput(f\"tao-client {model_name} dataset-create --dataset_type object_detection --dataset_format raw\")\n",
    "print(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rsync -ah --info=progress2 {data_dir}/images ~/shared/users/{identity['user_id']}/datasets/{dataset_id}/\n",
    "!rsync -ah --info=progress2 {data_dir}/labels ~/shared/users/{identity['user_id']}/datasets/{dataset_id}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = os.path.join(home, 'shared', 'users', os.environ['USER'], 'datasets', '*', 'metadata.json')\n",
    "\n",
    "datasets = []\n",
    "for metadata_path in glob.glob(pattern):\n",
    "    with open(metadata_path, 'r') as metadata_file:\n",
    "        datasets.append(json.load(metadata_file))\n",
    "\n",
    "print(json.dumps(datasets, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model_id = subprocess.getoutput(f\"tao-client {model_name} model-create --network_arch {model_name}\")\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign datast\n",
    "metadata_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'metadata.json')\n",
    "\n",
    "with open(metadata_path , \"r\") as metadata_file:\n",
    "    metadata = json.load(metadata_file)\n",
    "\n",
    "metadata[\"inference_dataset\"] = dataset_id\n",
    "\n",
    "with open(metadata_path, \"w\") as metadata_file:\n",
    "    json.dump(metadata, metadata_file, indent=2)\n",
    "\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set action specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model specs\n",
    "! tao-client {model_name} model-action-defaults --id {model_id} --action {action} | tee ~/shared/users/{os.environ['USER']}/models/{model_id}/specs/{action}.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set specs\n",
    "specs_path = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'specs', f'{action}.json')\n",
    "\n",
    "with open(specs_path , \"r\") as specs_file:\n",
    "    specs = json.load(specs_file)\n",
    "\n",
    "with open(specs_path, \"w\") as specs_file:\n",
    "    json.dump(specs, specs_file, indent=2)\n",
    "\n",
    "print(json.dumps(specs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the data analytics action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "analyze_job_id = subprocess.getoutput(f\"tao-client {model_name} execute-action --id {model_id} --action {action}\")\n",
    "print(analyze_job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status (the file won't exist until the backend Toolkit container is running -- can take several minutes)\n",
    "logs_dir = os.path.join(home, 'shared', 'users', os.environ['USER'], 'models', model_id, 'logs')\n",
    "log_file = f\"{analyze_job_id}.txt\"\n",
    "\n",
    "my_tail(logs_dir, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf ~/shared/users/{os.environ['USER']}/models/{model_id}\n",
    "! echo DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf ~/shared/users/{os.environ['USER']}/datasets/{dataset_id}\n",
    "! echo DONE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
