{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to demonstrate Image Segmentation workflow\n",
    "\n",
    "Transfer learning is the process of transferring learned features from one application to another. It is a commonly used training technique where you use a model trained on one task and re-train to use it on a different task. Train Adapt Optimize (TAO) Toolkit  is a simple and easy-to-use Python based AI toolkit for taking purpose-built AI models and customizing them with users' own data.\n",
    "\n",
    "![image](https://developer.nvidia.com/sites/default/files/akamai/TAO/tlt-tao-toolkit-bring-your-own-model-diagram.png)\n",
    "\n",
    "### Sample prediction for an Semantic Segmentation model - Unet, Segformer\n",
    "<img align=\"center\" src=\"../example_images/sample_semantic_segmentation.jpg\">\n",
    "\n",
    "### Sample prediction for an Instance Segmentation model - Mask-RCNN\n",
    "<img align=\"center\" width=\"800\" src=\"https://developer.nvidia.com/sites/default/files/akamai/TLT/test.jpg\">\n",
    "\n",
    "### The workflow in a nutshell\n",
    "\n",
    "- Creating a dataset\n",
    "- Upload dataset to the service\n",
    "- Running dataset convert (For Mask-RCNN)\n",
    "- Getting a PTM from NGC\n",
    "- Model Actions\n",
    "    - Train (Normal/AutoML)\n",
    "    - Evaluate\n",
    "    - Prune, retrain\n",
    "    - Export\n",
    "    - TAO-Deploy\n",
    "    - Inference on TAO\n",
    "    - Inference on TRT\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Create datasets ](#head-1)\n",
    "1. [List the created datasets](#head-2)\n",
    "1. [Dataset convert Action](#head-3)\n",
    "1. [Create model ](#head-4)\n",
    "1. [List models](#head-5)\n",
    "1. [Assign train, eval datasets](#head-6)\n",
    "1. [Assign PTM](#head-7)\n",
    "1. [View hyperparameters that are enabled by default](#head-8)\n",
    "1. [Set AutoML related configurations](#head-9)\n",
    "1. [Actions](#head-10)\n",
    "1. [Train](#head-11)\n",
    "1. [Evaluate](#head-12)\n",
    "1. [Optimize: Apply specs for prune](#head-14)\n",
    "1. [Optimize: Apply specs for retrain](#head-15)\n",
    "1. [Optimize: Run actions](#head-16)\n",
    "1. [Export](#head-17)\n",
    "1. [TRT Engine generation using TAO-Deploy](#head-19)\n",
    "1. [TAO inference](#head-20)\n",
    "1. [TRT inference](#head-21)\n",
    "\n",
    "### Requirements\n",
    "Please find the server requirements [here](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_setup.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "import uuid\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIXME\n",
    "\n",
    "1. Assign a model_name in FIXME 1\n",
    "2. Assign a workdir in FIXME 2\n",
    "3. Assign the ip_address and port_number in FIXME 3 ([info](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_api/api_rest_api.html))\n",
    "4. Assign the ngc_api_key variable in FIXME 4\n",
    "5. (Optional) Enable AutoML if needed in FIXME 5\n",
    "6. Choose between default and custom dataset in FIXME 6\n",
    "7. Assign path of DATA_DIR in FIXME 7\n",
    "8. Choose between Bayesian and Hyperband automl_algorithm in FIXME 8 (If automl was enabled in FIXME5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model_name workspaces and other variables\n",
    "# Available models (#FIXME 1):\n",
    "# 1. mask_rcnn - https://docs.nvidia.com/tao/tao-toolkit/text/instance_segmentation/mask_rcnn.html\n",
    "# 2. segformer - https://docs.nvidia.com/tao/tao-toolkit/text/semantic_segmentation/segformer.html\n",
    "# 3. unet - https://docs.nvidia.com/tao/tao-toolkit/text/semantic_segmentation/unet.html\n",
    "\n",
    "model_name = \"mask_rcnn\" # FIXME1 (Add the model name from the above mentioned list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = \"workdir_segmentation\" # FIXME2\n",
    "host_url = \"http://<ip_address>:<port_number>\" # FIXME3 example: https://10.137.149.22:32334\n",
    "# In host machine, node ip_address and port number can be obtained as follows,\n",
    "# ip_address: hostname -i\n",
    "# port_number: kubectl get service ingress-nginx-controller -o jsonpath='{.spec.ports[0].nodePort}'\n",
    "ngc_api_key = \"<ngc_api_key>\" # FIXME4 example: (Add NGC API key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_enabled = False # FIXME5 set to TRUE if you want to run automl for the model chosen in the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange NGC_API_KEY for JWT\n",
    "response = requests.get(f\"{host_url}/api/v1/login/{ngc_api_key}\")\n",
    "user_id = response.json()[\"user_id\"]\n",
    "print(\"User ID\",user_id)\n",
    "token = response.json()[\"token\"]\n",
    "print(\"JWT\",token)\n",
    "\n",
    "# Set base URL\n",
    "base_url = f\"{host_url}/api/v1/user/{user_id}\"\n",
    "print(\"API Calls will be forwarded to\",base_url)\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating workdir\n",
    "if not os.path.isdir(workdir):\n",
    "    os.makedirs(workdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets <a class=\"anchor\" id=\"head-1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instance Segmentation:**\n",
    "We will be using the `COCO dataset` for Instance segmentation - MaskRCNN. `download_coco.sh` script from dataset prepare will be used to download and unzip the coco2017 dataset from [here](https://cocodataset.org/#download)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If using custom dataset; it should follow this dataset structure**\n",
    "```\n",
    "DATA_DIR\n",
    "├── annotations.json\n",
    "├── images\n",
    "    ├── image_name_1.jpg\n",
    "    ├── image_name_2.jpg\n",
    "    ├── ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic Segmentation:**\n",
    "We will be using the `ISBI Challenge: Segmentation of neuronal structures in EM stacks dataset` for the binary segmentation tutorial (Unet and Segformer). Please access the open source repo [here](https://github.com/alexklibisz/isbi-2012/tree/master/data) to download the data. The data is in .tif format. Copy the train-labels.tif, train-volume.tif, test-volume.tif files to `DATA_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If using custom dataset; it should follow this dataset structure**\n",
    "```\n",
    "DATA_DIR\n",
    "├── images\n",
    "│   ├── test\n",
    "│   │   ├── image_0.png\n",
    "│   │   ├── image_1.png\n",
    "|   |   ├── ...\n",
    "│   ├── train\n",
    "│   │   ├── image_2.png\n",
    "│   │   ├── image_3.png\n",
    "|   |   ├── ...\n",
    "│   └── val\n",
    "│       ├── image_4.png\n",
    "│       ├── image_5.png\n",
    "|       ├── ...\n",
    "├── masks\n",
    "    ├── train\n",
    "    │   ├── image_2.png\n",
    "    │   ├── image_3.png\n",
    "    |   ├── ...\n",
    "    └── val\n",
    "        ├── image_4.png\n",
    "        ├── image_5.png\n",
    "        ├── ...\n",
    "\n",
    "```\n",
    "The filename should match for images and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_to_be_used = \"default\" #FIXME6 #default/custom; default for the dataset used in this tutorial notebook; custom for a different dataset\n",
    "DATA_DIR = model_name # FIXME7\n",
    "os.environ['DATA_DIR']= DATA_DIR\n",
    "!mkdir -p $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"mask_rcnn\" and dataset_to_be_used == \"default\":\n",
    "    !bash dataset_prepare/coco/download_coco.sh $DATA_DIR\n",
    "    # Remove existing data\n",
    "    !rm -rf $DATA_DIR/train2017/images\n",
    "    !rm -rf $DATA_DIR/val2017/images\n",
    "    # Rearrange data in the required format\n",
    "    !mkdir -p $DATA_DIR/train2017/\n",
    "    !mkdir -p $DATA_DIR/val2017/\n",
    "    !mv $DATA_DIR/raw-data/train2017 $DATA_DIR/train2017/images\n",
    "    !mv $DATA_DIR/raw-data/annotations/instances_train2017.json $DATA_DIR/train2017/annotations.json\n",
    "    !mv $DATA_DIR/raw-data/val2017 $DATA_DIR/val2017/images\n",
    "    !mv $DATA_DIR/raw-data/annotations/instances_val2017.json $DATA_DIR/val2017/annotations.json\n",
    "    !cp dataset_prepare/coco/label_map.txt $DATA_DIR/train2017/\n",
    "    !cp dataset_prepare/coco/label_map.txt $DATA_DIR/val2017/\n",
    "    \n",
    "# For unet/segformer you have to manually download from the github link https://github.com/alexklibisz/isbi-2012/tree/master/data and place it in $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the downloaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"mask_rcnn\":\n",
    "    !if [ ! -d $DATA_DIR/train2017/images ]; then echo 'Images folder not found'; else echo 'Found images folder';fi\n",
    "    !if [ ! -f $DATA_DIR/train2017/annotations.json ]; then echo 'annotations file not found'; else echo 'Found annotations file';fi\n",
    "    !if [ ! -d $DATA_DIR/val2017/images ]; then echo 'Images folder not found'; else echo 'Found images folder';fi\n",
    "    !if [ ! -f $DATA_DIR/val2017/annotations.json ]; then echo 'annotations file not found'; else echo 'Found annotations file';fi\n",
    "if model_name in (\"unet\",\"segformer\") and dataset_to_be_used == \"default\":\n",
    "    !if [ ! -f $DATA_DIR/train-volume.tif ]; then echo 'train-volume.tif file not found, please download.'; else echo 'Found test-volume.tif file.';fi\n",
    "    !if [ ! -f $DATA_DIR/train-labels.tif ]; then echo 'train-labels file not found, please download.'; else echo 'Found train-labels.tif file.';fi\n",
    "    !if [ ! -f $DATA_DIR/test-volume.tif ]; then echo 'train-volume.tif file not found, please download.'; else echo 'Found train-volume.tif file.';fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name in (\"unet\",\"segformer\"):\n",
    "    if dataset_to_be_used == \"default\":\n",
    "        !python3 -m pip install Pillow opencv-python numpy\n",
    "        !bash dataset_prepare/unet/prepare_data.sh $DATA_DIR # creates images and masks from the tif files\n",
    "    !tar -czf isbi_data.tar.gz -C $DATA_DIR .\n",
    "elif model_name == \"mask_rcnn\":\n",
    "    !tar -C $DATA_DIR/train2017 -czf coco_train.tar.gz images annotations.json label_map.txt\n",
    "    !tar -C $DATA_DIR/val2017 -czf coco_val.tar.gz images annotations.json label_map.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name in (\"unet\",\"segformer\"):\n",
    "    train_dataset_path = \"isbi_data.tar.gz\"\n",
    "    eval_dataset_path = \"isbi_data.tar.gz\"\n",
    "elif model_name == \"mask_rcnn\":\n",
    "    train_dataset_path = \"coco_train.tar.gz\"\n",
    "    eval_dataset_path = \"coco_val.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create train dataset\n",
    "if model_name in (\"unet\",\"segformer\"):\n",
    "    ds_type = \"semantic_segmentation\"\n",
    "    ds_format = \"unet\"\n",
    "elif model_name == \"mask_rcnn\":\n",
    "    ds_type = \"instance_segmentation\"\n",
    "    ds_format = \"coco\"\n",
    "\n",
    "data = json.dumps({\"type\":ds_type,\"format\":ds_format})\n",
    "\n",
    "endpoint = f\"{base_url}/dataset\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update\n",
    "dataset_information = {\"name\":\"Train dataset\",\n",
    "                       \"description\":\"My train dataset\"}\n",
    "data = json.dumps(dataset_information)\n",
    "\n",
    "endpoint = f\"{base_url}/dataset/{dataset_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload\n",
    "files = [(\"file\",open(train_dataset_path,\"rb\"))]\n",
    "\n",
    "endpoint = f\"{base_url}/dataset/{dataset_id}/upload\"\n",
    "\n",
    "response = requests.post(endpoint, files=files, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create eval dataset\n",
    "data = json.dumps({\"type\":ds_type,\"format\":ds_format})\n",
    "\n",
    "endpoint = f\"{base_url}/dataset\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "eval_dataset_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update\n",
    "dataset_information = {\"name\":\"Evaluation dataset\",\n",
    "                       \"description\":\"My eval dataset\"}\n",
    "data = json.dumps(dataset_information)\n",
    "\n",
    "endpoint = f\"{base_url}/dataset/{eval_dataset_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload\n",
    "files = [(\"file\",open(eval_dataset_path,\"rb\"))]\n",
    "\n",
    "endpoint = f\"{base_url}/dataset/{eval_dataset_id}/upload\"\n",
    "\n",
    "response = requests.post(endpoint, files=files, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the created datasets <a class=\"anchor\" id=\"head-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/dataset\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "# print(response.json()) ## Uncomment for verbose list output\n",
    "print(\"id\\t\\t\\t\\t\\t type\\t\\t\\t format\\t\\t name\")\n",
    "for rsp in response.json():\n",
    "    print(rsp[\"id\"],\"\\t\",rsp[\"type\"],\"\\t\",rsp[\"format\"],\"\\t\\t\",rsp[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset convert Action <a class=\"anchor\" id=\"head-3\"></a>\n",
    "#### Run dataset convert only for coco data format, skip to Create model for unet data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ds_format == \"coco\" and model_name != \"segformer\":\n",
    "    # Get default spec schema\n",
    "    endpoint = f\"{base_url}/dataset/{dataset_id}/specs/convert/schema\"\n",
    "\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "    #print(response)\n",
    "    #print(response.json()) ## Uncomment for verbose schema\n",
    "\n",
    "    specs = response.json()[\"default\"]\n",
    "    print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ds_format == \"coco\" and model_name != \"segformer\":\n",
    "    # Apply changes\n",
    "    specs[\"dataset_convert\"][\"num_shards\"] = 256\n",
    "    specs[\"dataset_convert\"][\"tag\"] = \"train\"\n",
    "    print(specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ds_format == \"coco\" and model_name != \"segformer\":\n",
    "    # Post spec\n",
    "    data = json.dumps(specs)\n",
    "\n",
    "    endpoint = f\"{base_url}/dataset/{dataset_id}/specs/convert\"\n",
    "\n",
    "    response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ds_format == \"coco\" and model_name != \"segformer\":\n",
    "    # Run action\n",
    "    parent = None\n",
    "    actions = [\"convert\"]\n",
    "    data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "    endpoint = f\"{base_url}/dataset/{dataset_id}/job\"\n",
    "\n",
    "    response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "\n",
    "    ds_convert_id = response.json()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ds_format == \"coco\" and model_name != \"segformer\":\n",
    "    # Monitor job status by repeatedly running this cell\n",
    "    job_id = ds_convert_id\n",
    "    endpoint = f\"{base_url}/dataset/{dataset_id}/job/{job_id}\"\n",
    "\n",
    "    while True:    \n",
    "        clear_output(wait=True)\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        print(response)\n",
    "        print(response.json())\n",
    "        if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "            break\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ds_format == \"coco\" and model_name != \"segformer\":\n",
    "    # Now, repeat the same for the eval dataset\n",
    "    # Get default spec schema\n",
    "    endpoint = f\"{base_url}/dataset/{eval_dataset_id}/specs/convert/schema\"\n",
    "\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    #print(response.json()) ## Uncomment for verbose schema\n",
    "    specs = response.json()[\"default\"]\n",
    "    print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ds_format == \"coco\" and model_name != \"segformer\":\n",
    "    ## Apply changes\n",
    "    specs[\"dataset_convert\"][\"num_shards\"] = 256\n",
    "    specs[\"dataset_convert\"][\"tag\"] = \"val\"\n",
    "    print(specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ds_format == \"coco\" and model_name != \"segformer\":\n",
    "    # Post spec\n",
    "    data = json.dumps(specs)\n",
    "\n",
    "    endpoint = f\"{base_url}/dataset/{eval_dataset_id}/specs/convert\"\n",
    "\n",
    "    response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ds_format == \"coco\" and model_name != \"segformer\":\n",
    "    # Run action\n",
    "    parent = None\n",
    "    actions = [\"convert\"]\n",
    "    data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "    endpoint = f\"{base_url}/dataset/{eval_dataset_id}/job\"\n",
    "\n",
    "    response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "\n",
    "    eval_ds_convert_id = response.json()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ds_format == \"coco\" and model_name != \"segformer\":\n",
    "    # Monitor job status by repeatedly running this cell\n",
    "    job_id = eval_ds_convert_id\n",
    "    endpoint = f\"{base_url}/dataset/{eval_dataset_id}/job/{job_id}\"\n",
    "\n",
    "    while True:    \n",
    "        clear_output(wait=True)\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        print(response)\n",
    "        print(response.json())\n",
    "        if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "            break\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model <a class=\"anchor\" id=\"head-4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"segformer\":\n",
    "    encode_key = \"nvidia_tao\"\n",
    "else:\n",
    "    encode_key = \"tlt_encode\"\n",
    "data = json.dumps({\"network_arch\":model_name,\"encryption_key\":encode_key})\n",
    "\n",
    "endpoint = f\"{base_url}/model\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "model_id = response.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List models <a class=\"anchor\" id=\"head-5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "endpoint = f\"{base_url}/model\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "# print(response.json()) ## Uncomment for verbose list output\n",
    "print(\"model id\\t\\t\\t     network architecture\")\n",
    "for rsp in response.json():\n",
    "    print(rsp[\"id\"],rsp[\"network_arch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign train, eval datasets <a class=\"anchor\" id=\"head-6\"></a>\n",
    "\n",
    "- Note: make sure the order for train_datasets is [source ID, target ID]\n",
    "- eval_dataset is kept same as target for demo purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_information = {\"train_datasets\":[dataset_id],\n",
    "                       \"eval_dataset\":eval_dataset_id,\n",
    "                       \"inference_dataset\":eval_dataset_id,\n",
    "                       \"calibration_dataset\":dataset_id}\n",
    "data = json.dumps(dataset_information)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}\"\n",
    "\n",
    "response = requests.patch(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign PTM <a class=\"anchor\" id=\"head-7\"></a>\n",
    "\n",
    "Search for the PTM on NGC for the Segmentation model chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List all pretrained models for the chosen network architecture\n",
    "model_list = f\"{base_url}/model\"\n",
    "response = requests.get(model_list, headers=headers)\n",
    "\n",
    "response_json = response.json()\n",
    "\n",
    "for rsp in response_json:\n",
    "    if rsp[\"network_arch\"] == model_name:\n",
    "        if \"encryption_key\" not in rsp.keys():\n",
    "            print(f'PTM Name: {rsp[\"name\"]}; PTM version: {rsp[\"version\"]}; NGC PATH: {rsp[\"ngc_path\"]}; Additional info: {rsp[\"additional_id_info\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning pretrained models to different networks\n",
    "# From the output of previous cell make the appropriate changes to this map if you want to change the default PTM backbone.\n",
    "# Changing the default backbone here requires changing default spec/config during train/eval etc like for example\n",
    "# If you are changing the ptm to resnet34, then you have to modify the config key num_layers if it exists to 34 manually\n",
    "pretrained_map = {\"mask_rcnn\" : \"pretrained_instance_segmentation:resnet18\",\n",
    "                  \"segformer\" : \"pretrained_segformer_imagenet:fan_hybrid_tiny\",\n",
    "                  \"unet\" : \"pretrained_semantic_segmentation:resnet18\"}\n",
    "no_ptm_models = set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained model\n",
    "if model_name not in no_ptm_models:\n",
    "    model_list = f\"{base_url}/model\"\n",
    "    response = requests.get(model_list, headers=headers)\n",
    "\n",
    "    response_json = response.json()\n",
    "\n",
    "    # Search for ptm with given ngc path\n",
    "    ptm = []\n",
    "    for rsp in response_json:\n",
    "        if rsp[\"network_arch\"] == model_name and rsp[\"ngc_path\"].endswith(pretrained_map[model_name]):\n",
    "            ptm_id = rsp[\"id\"]\n",
    "            ptm = [ptm_id]\n",
    "            print(\"Metadata for model with requested NGC Path\")\n",
    "            print(rsp)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_name not in no_ptm_models:\n",
    "    ptm_information = {\"ptm\":ptm}\n",
    "    data = json.dumps(ptm_information)\n",
    "\n",
    "    endpoint = f\"{base_url}/model/{model_id}\"\n",
    "\n",
    "    response = requests.patch(endpoint, data=data, headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View hyperparameters that are enabled for AutoML by default <a class=\"anchor\" id=\"head-8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if automl_enabled:\n",
    "    # Get default spec schema\n",
    "    endpoint = f\"{base_url}/model/{model_id}/specs/train/schema\"\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    specs = response.json()[\"automl_default_parameters\"]\n",
    "    print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set AutoML related configurations <a class=\"anchor\" id=\"head-9\"></a>\n",
    "Refer to these hyper-links to see the parameters supported by each network and add more parameters if necessary in addition to the default automl enabled parameters: \n",
    "\n",
    "[Mask RCNN](https://github.com/NVIDIA/tao_front_end_services/tree/main/api/specs_utils/specs/mask_rcnn/mask_rcnn%20-%20train.csv),\n",
    "[Segformer](https://github.com/NVIDIA/tao_front_end_services/tree/main/api/specs_utils/specs/segformer/segformer%20-%20train.csv),\n",
    "[Unet](https://github.com/NVIDIA/tao_front_end_services/tree/main/api/specs_utils/specs/unet/unet%20-%20train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if automl_enabled:\n",
    "    # Choose automl algorithm between \"Bayesian\" and \"HyperBand\".\n",
    "    automl_algorithm=\"Bayesian\" # FIXME8 example: Bayesian/HyperBand\n",
    "\n",
    "    metric=\"kpi\" #don't change, more metrics will be supported in the future\n",
    "\n",
    "    additional_automl_parameters = [] #Refer to parameter list mentioned in the above links and add any extra parameter in addition to the default enabled ones\n",
    "    remove_default_automl_parameters = [] #Remove any hyperparameters that are enabled by default for AutoML\n",
    "\n",
    "    automl_information = {\"automl_enabled\":automl_enabled,\n",
    "                          \"automl_algorithm\":automl_algorithm,\n",
    "                          \"epoch_multiplier\": 1, # Will be considered for Hyperband only\n",
    "                          \"metric\":metric,\n",
    "                          \"automl_add_hyperparameters\":str(additional_automl_parameters),\n",
    "                          \"automl_remove_hyperparameters\":str(remove_default_automl_parameters)\n",
    "                         }\n",
    "    data = json.dumps(automl_information)\n",
    "\n",
    "    endpoint = f\"{base_url}/model/{model_id}\"\n",
    "\n",
    "    response = requests.patch(endpoint, data=data, headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions <a class=\"anchor\" id=\"head-10\"></a>\n",
    "\n",
    "For all actions:\n",
    "1. Get default spec schema and derive the default values\n",
    "2. Modify defaults if needed\n",
    "3. Post spec dictionary to the service\n",
    "4. Run model action\n",
    "5. Monitor job using retrieve\n",
    "6. Download results using job download endpoint (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_map = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train <a class=\"anchor\" id=\"head-11\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/train/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override any of the parameters listed in the previous cell as required\n",
    "if model_name == \"mask_rcnn\":\n",
    "    # For each network the parameter key might be different for example, in mask_rcnn training duration is determined by num_epochs or total_steps\n",
    "    specs[\"num_epochs\"] = 5\n",
    "    specs[\"gpus\"] = 1\n",
    "    specs[\"num_examples_per_epoch\"] = 5000 # Set it as the number of images in your dataset for mask-rcnn / num of GPU's\n",
    "elif model_name == \"unet\":\n",
    "    specs[\"training_config\"][\"epochs\"] = 50\n",
    "    specs[\"gpus\"] = 1\n",
    "elif model_name == \"segformer\":\n",
    "    specs[\"dataset\"][\"batch_size\"] = 4\n",
    "    specs[\"train\"][\"max_iters\"] = 1000\n",
    "    specs[\"train\"][\"num_gpus\"] = 1\n",
    "    specs[\"gpus\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/train\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = None\n",
    "actions = [\"train\"]\n",
    "data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"train\"] = response.json()[0]\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "# For automl: Training times for different models benchmarked on 1 GPU V100 machine can be found here: https://docs.nvidia.com/tao/tao-toolkit/text/automl/automl.html#results-of-automl-experiments\n",
    "\n",
    "job_id = job_map['train']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    if \"error_desc\" in response.json().keys() and response.json()[\"error_desc\"] in (\"Job not found\", \"No AutoML run found\"):\n",
    "        print(\"Job is being created\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To Stop an AutoML JOB\n",
    "#    1. Stop the 'Monitor job status by repeatedly running this cell' cell (the cell right before this cell) manually\n",
    "#    2. Uncomment the snippet in the next cell and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if automl_enabled:\n",
    "#     job_id = job_map['train']\n",
    "#     endpoint = f\"{base_url}/model/{model_id}/job/{job_id}/cancel\"\n",
    "\n",
    "#     response = requests.post(endpoint, headers=headers)\n",
    "\n",
    "#     print(response)\n",
    "#     print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resume AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the below snippet if you want to resume an already stopped AutoML job and then run the 'Monitor job status by repeatedly running this cell' cell above (4th cell above from this cell)\n",
    "# if automl_enabled:\n",
    "#     job_id = job_map['train']\n",
    "#     endpoint = f\"{base_url}/model/{model_id}/job/{job_id}/resume\"\n",
    "\n",
    "#     response = requests.post(endpoint, headers=headers)\n",
    "\n",
    "#     print(response)\n",
    "#     print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download job contents once the above job shows \"Done\" status\n",
    "# Download output of train job (Note: will take time)\n",
    "job_id = job_map[\"train\"]\n",
    "endpoint = f'{base_url}/model/{model_id}/job/{job_id}/download'\n",
    "\n",
    "# Save\n",
    "temptar = f'{job_id}.tar.gz'\n",
    "with requests.get(endpoint, headers=headers, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(temptar, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "print(\"Untarring\")\n",
    "# Untar to destination\n",
    "tar_command = f'tar -xf {temptar} -C {workdir}/'\n",
    "os.system(tar_command)\n",
    "os.remove(temptar)\n",
    "print(f\"Results at {workdir}/{job_id}\")\n",
    "model_downloaded_path = f\"{workdir}/{job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the checkpoints generated for the training job and for automl jobs, in addition view: best performing model's config and the results of all automl experiments\n",
    "\n",
    "if automl_enabled:\n",
    "    !python3 -m pip install pandas==1.5.1\n",
    "    import pandas as pd\n",
    "    model_downloaded_path = f\"{model_downloaded_path}/best_model\"\n",
    "\n",
    "if os.path.exists(model_downloaded_path):        \n",
    "    #List the binary model file\n",
    "    print(\"\\nCheckpoints for the training experiment\")\n",
    "    if os.path.exists(model_downloaded_path+\"/train/weights\") and len(os.listdir(model_downloaded_path+\"/train/weights\")) > 0:\n",
    "        print(f\"Folder: {model_downloaded_path}/train/weights\")\n",
    "        print(\"Files:\", os.listdir(model_downloaded_path+\"/train/weights\"))\n",
    "    elif os.path.exists(model_downloaded_path+\"/weights\") and len(os.listdir(model_downloaded_path+\"/weights\")) > 0:\n",
    "        print(f\"Folder: {model_downloaded_path}/weights\")\n",
    "        print(\"Files:\", os.listdir(model_downloaded_path+\"/weights\"))\n",
    "    else:\n",
    "        print(f\"Folder: {model_downloaded_path}\")\n",
    "        print(\"Files:\", os.listdir(model_downloaded_path))\n",
    "\n",
    "    if automl_enabled:\n",
    "        experiment_artifacts = json.load(open(f\"{model_downloaded_path}/controller.json\",\"r\"))\n",
    "        data_frame = pd.DataFrame(experiment_artifacts)\n",
    "        # Print experiment id/number and the corresponding result\n",
    "        print(\"\\nResults of all experiments\")\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):\n",
    "            print(data_frame[[\"id\",\"result\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate <a class=\"anchor\" id=\"head-12\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/evaluate/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply changes to the specs if required\n",
    "if model_name == \"segformer\":\n",
    "    specs[\"dataset\"][\"batch_size\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/evaluate\"\n",
    "\n",
    "response = requests.post(endpoint,data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train\"]\n",
    "actions = [\"evaluate\"]\n",
    "data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"evaluate\"] = response.json()[0]\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map['evaluate']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize <a class=\"anchor\" id=\"head-13\"></a>\n",
    "\n",
    "- We optimize the trained model by pruning and retraining in the following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply specs for prune <a class=\"anchor\" id=\"head-14\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name != \"segformer\":\n",
    "    # Get default spec schema\n",
    "    endpoint = f\"{base_url}/model/{model_id}/specs/prune/schema\"\n",
    "\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    #print(response.json()) ## Uncomment for verbose schema\n",
    "    specs = response.json()[\"default\"]\n",
    "    print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name != \"segformer\":\n",
    "    # Apply changes to specs if necessary like\n",
    "    specs[\"pruning_threshold\"] = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name != \"segformer\":\n",
    "    # Post spec\n",
    "    data = json.dumps(specs)\n",
    "\n",
    "    endpoint = f\"{base_url}/model/{model_id}/specs/prune\"\n",
    "\n",
    "    response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply specs for retrain <a class=\"anchor\" id=\"head-15\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_name != \"segformer\":\n",
    "    # Get default spec schema\n",
    "    endpoint = f\"{base_url}/model/{model_id}/specs/retrain/schema\"\n",
    "\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    #print(response.json()) ## Uncomment for verbose schema\n",
    "    specs = response.json()[\"default\"]\n",
    "\n",
    "    print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override any of the parameters listed in the previous cell as required\n",
    "if model_name == \"mask_rcnn\":\n",
    "    # For each network the parameter key might be different for example, in mask_rcnn training duration is determined by num_epochs or total_steps\n",
    "    specs[\"num_epochs\"] = 5\n",
    "    specs[\"gpus\"] = 1\n",
    "    specs[\"num_examples_per_epoch\"] = 5000 # Set it as the number of images in your dataset for mask-rcnn / num of GPU's\n",
    "elif model_name == \"unet\":\n",
    "    specs[\"training_config\"][\"epochs\"] = 50\n",
    "    specs[\"gpus\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_name != \"segformer\":\n",
    "    # Post spec\n",
    "    data = json.dumps(specs)\n",
    "\n",
    "    endpoint = f\"{base_url}/model/{model_id}/specs/retrain\"\n",
    "\n",
    "    response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Actions <a class=\"anchor\" id=\"head-16\"></a>\n",
    "\n",
    "We use the API's job chaining feature to prune, retrain and evaluate the retrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name != \"segformer\":\n",
    "    # Run actions\n",
    "    parent = job_map[\"train\"]\n",
    "    actions = [\"prune\",\"retrain\",\"evaluate\"]\n",
    "    data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "    endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "    response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "\n",
    "    job_map[\"prune\"] = response.json()[0]\n",
    "    job_map[\"retrain\"] = response.json()[1]\n",
    "    job_map[\"eval_retrain\"] = response.json()[2]\n",
    "    print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_name != \"segformer\":\n",
    "    # Monitor job status by repeatedly running this cell (prune)\n",
    "    job_id = job_map['prune']\n",
    "    endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "    while True:    \n",
    "        clear_output(wait=True)\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        print(response)\n",
    "        print(response.json())\n",
    "        if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "            break\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_name != \"segformer\":\n",
    "    # Monitor job status by repeatedly running this cell (retrain)\n",
    "    job_id = job_map['retrain']\n",
    "    endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "    while True:    \n",
    "        clear_output(wait=True)\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        print(response)\n",
    "        print(response.json())\n",
    "        if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "            break\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name != \"segformer\":\n",
    "    # Monitor job status by repeatedly running this cell (evaluate)\n",
    "    job_id = job_map['eval_retrain']\n",
    "    endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "    while True:    \n",
    "        clear_output(wait=True)\n",
    "        response = requests.get(endpoint, headers=headers)\n",
    "        print(response)\n",
    "        print(response.json())\n",
    "        if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "            break\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional cancel job - for jobs that are pending/running (retrain)\n",
    "\n",
    "# job_id = job_map['retrain']\n",
    "# endpoint = f\"{base_url}/model/{model_id}/job/{job_id}/cancel\"\n",
    "\n",
    "# response = requests.post(endpoint, headers=headers)\n",
    "\n",
    "# print(response)\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional delete job - for jobs that are error/done (retrain)\n",
    "\n",
    "# job_id = job_map['retrain']\n",
    "# endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "# response = requests.delete(endpoint, headers=headers)\n",
    "\n",
    "# print(response)\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export <a class=\"anchor\" id=\"head-15\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/export/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/export\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train\"]\n",
    "actions = [\"export\"]\n",
    "data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"export\"] = response.json()[0]\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map['export']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download job contents once the above job shows \"Done\" status\n",
    "job_id = job_map[\"export\"]\n",
    "endpoint = f'{base_url}/model/{model_id}/job/{job_id}/download'\n",
    "\n",
    "# Save\n",
    "temptar = f'{job_id}.tar.gz'\n",
    "with requests.get(endpoint, headers=headers, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(temptar, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "print(\"Untarring\")\n",
    "# Untar to destination\n",
    "tar_command = f'tar -xf {temptar} -C {workdir}/'\n",
    "os.system(tar_command)\n",
    "os.remove(temptar)\n",
    "print(f\"Results at {workdir}/{job_id}\")\n",
    "model_downloaded_path = f\"{workdir}/{job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for the generated .onnx file\n",
    "!ls {model_downloaded_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRT Engine generation using TAO-Deploy <a class=\"anchor\" id=\"head-19\"></a>\n",
    "\n",
    "- Here, we use the exported model to generate trt engine on the target platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/gen_trt_engine/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply changes to specs dictionary in this cell if required\n",
    "if model_name == \"segformer\":\n",
    "    specs[\"gen_trt_engine\"][\"tensorrt\"][\"data_type\"] = \"fp16\"\n",
    "else:\n",
    "    specs[\"data_type\"] = \"int8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/gen_trt_engine\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"export\"]\n",
    "actions = [\"gen_trt_engine\"]\n",
    "data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"model_gen_trt_engine\"] = response.json()[0]\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map['model_gen_trt_engine']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAO inference <a class=\"anchor\" id=\"head-20\"></a>\n",
    "\n",
    "- Run inference on a set of images using the .tlt model created at train step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/inference/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply changes to specs if necessary\n",
    "if model_name == \"segformer\":\n",
    "    specs[\"dataset\"][\"batch_size\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/inference\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"train\"]\n",
    "actions = [\"inference\"]\n",
    "data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"inference_tlt\"] = response.json()[0]\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map['inference_tlt']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download job contents once the above job shows \"Done\" status\n",
    "job_id = job_map[\"inference_tlt\"]\n",
    "endpoint = f'{base_url}/model/{model_id}/job/{job_id}/download'\n",
    "\n",
    "# Save\n",
    "temptar = f'{job_id}.tar.gz'\n",
    "with requests.get(endpoint, headers=headers, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(temptar, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "print(\"Untarring\")\n",
    "# Untar to destination\n",
    "tar_command = f'tar -xf {temptar} -C {workdir}/'\n",
    "os.system(tar_command)\n",
    "os.remove(temptar)\n",
    "print(f\"Results at {workdir}/{job_id}\")\n",
    "inference_out_path = f\"{workdir}/{job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import glob\n",
    "sample_image = (glob.glob(f\"{inference_out_path}/**/*.jpg\", recursive=True) + glob.glob(f\"{inference_out_path}/**/*.png\", recursive=True))[0]\n",
    "Image(filename=sample_image) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRT inference <a class=\"anchor\" id=\"head-21\"></a>\n",
    "\n",
    "- Set batch size to the value used during trt engine generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get default spec schema\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/inference/schema\"\n",
    "\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "\n",
    "print(response)\n",
    "#print(response.json()) ## Uncomment for verbose schema\n",
    "specs = response.json()[\"default\"]\n",
    "print(json.dumps(specs, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply changes to specs if necessary\n",
    "if model_name == \"segformer\":\n",
    "    specs[\"dataset\"][\"batch_size\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Post spec\n",
    "data = json.dumps(specs)\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/specs/inference\"\n",
    "\n",
    "response = requests.post(endpoint,data=data,headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(json.dumps(response.json(), sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run action\n",
    "parent = job_map[\"model_gen_trt_engine\"]\n",
    "actions = [\"inference\"]\n",
    "data = json.dumps({\"job\":parent,\"actions\":actions})\n",
    "\n",
    "endpoint = f\"{base_url}/model/{model_id}/job\"\n",
    "\n",
    "response = requests.post(endpoint, data=data, headers=headers)\n",
    "\n",
    "print(response)\n",
    "print(response.json())\n",
    "\n",
    "job_map[\"inference_trt\"] = response.json()[0]\n",
    "print(job_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Monitor job status by repeatedly running this cell\n",
    "job_id = job_map['inference_trt']\n",
    "endpoint = f\"{base_url}/model/{model_id}/job/{job_id}\"\n",
    "\n",
    "while True:    \n",
    "    clear_output(wait=True)\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    print(response)\n",
    "    print(response.json())\n",
    "    if response.json().get(\"status\") in [\"Done\",\"Error\"] or response.status_code not in (200,201):\n",
    "        break\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download job contents once the above job shows \"Done\" status\n",
    "job_id = job_map[\"inference_trt\"]\n",
    "endpoint = f'{base_url}/model/{model_id}/job/{job_id}/download'\n",
    "\n",
    "# Save\n",
    "temptar = f'{job_id}.tar.gz'\n",
    "with requests.get(endpoint, headers=headers, stream=True) as r:\n",
    "    r.raise_for_status()\n",
    "    with open(temptar, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "print(\"Untarring\")\n",
    "# Untar to destination\n",
    "tar_command = f'tar -xf {temptar} -C {workdir}/'\n",
    "os.system(tar_command)\n",
    "os.remove(temptar)\n",
    "print(f\"Results at {workdir}/{job_id}\")\n",
    "inference_out_path = f\"{workdir}/{job_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = (glob.glob(f\"{inference_out_path}/**/*.jpg\", recursive=True) + glob.glob(f\"{inference_out_path}/**/*.png\", recursive=True))[0]\n",
    "Image(filename=sample_image) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
